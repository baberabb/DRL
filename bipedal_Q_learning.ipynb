{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bipedal Q learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "APPENDIX 2: ADVANCED DQN: DOUBLE AND Deuling"
      ],
      "metadata": {
        "id": "eogopwSX3NHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Base code from https://github.com/Curt-Park/rainbow-is-all-you-need with adaptations from https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning/DeepQLearning, and https://www.youtube.com/watch?v=WHRQUZrxxGw&t=4786s"
      ],
      "metadata": {
        "id": "C0UQhnec3c67"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbEy9kstPVMt"
      },
      "outputs": [],
      "source": [
        "from torch.cuda import random\n",
        "!pip install Box2D\n",
        "import os\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "!pip install box2d-py\n",
        "!pip install gym[all]\n",
        "!pip install gym[Box_2D]\n",
        "!pip install git+https://github.com/ngc92/space-wrappers.git\n",
        "!pip install \"ray[rllib]\"==1.6\n",
        "\n",
        "import gym\n",
        "import space_wrappers\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import randint\n",
        "\n",
        "from collections import deque,namedtuple\n",
        "from typing import Any\n",
        "from random import sample, random\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from ray import tune\n",
        "import numpy as np\n",
        "import ray.rllib.agents.dqn as dqn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb"
      ],
      "metadata": {
        "id": "kmRPO4XBEm6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "QF9WgCjEEv8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import sample, random\n"
      ],
      "metadata": {
        "id": "GgNb5bMtZbpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "OLYydRVElOgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('LunarLander-v3').env\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "681UQtVzQK0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the Q network - with target"
      ],
      "metadata": {
        "id": "y1W8ZQySS_sW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tB8YytuGTBsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, inputs_states, output_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.inputs_states = inputs_states\n",
        "        self.output_actions = output_actions\n",
        "        self.fc1 = nn.Linear(inputs_states, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        #self.fc3 = nn.BatchNorm1d(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(256, output_actions)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.params(), lr=0.01)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        #x = self.fc3(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        #x = self.fc3(x)\n",
        "        x = self.fc4(x)\n",
        "\n",
        "        return x\n",
        "    \n",
        "    def training_step(self, state_transitions):\n",
        "      current_states = torch.stack([s.state for s in state_transitions])\n",
        "      rewards = torch.stack([s.reward for r in state_transitions])\n",
        "      next_states = torch.stack([s.new_state for ns in state_transitions])\n",
        "      if_done = torch.stack([0 if s.done else 1 for s in state_transitions])\n",
        "      actions = torch.stack([s.action for a in state_transitions])\n",
        "\n",
        "      \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M835_TPGS3pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env1 = gym.make('BipedalWalker-v3')\n",
        "wrapped = space_wrappers.DiscretizedActionWrapper(env1, 3)\n",
        "wrapped = space_wrappers.FlattenedActionWrapper(wrapped)\n",
        "env = wrapped\n",
        "\n",
        "#Initialize Network\n",
        "\n",
        "\n",
        "#gets trained every step\n",
        "q_evaluation = DQN(env)\n",
        "#gives predictions every step\n",
        "q_target = DQN(env)\n",
        "\n",
        "q_target.load_state_dict(q_evaluation.state_dict)\n",
        "\n",
        "\n",
        "#initialize buffer\n",
        "obs = env.reset()\n",
        "\n",
        "#get observations and tuple\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8towQt4DHF7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create action replay (taken from https://towardsdatascience.com/deep-q-network-with-pytorch-and-gym-to-solve-acrobot-game-d677836bda9b)"
      ],
      "metadata": {
        "id": "ggXpa1htUTRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))"
      ],
      "metadata": {
        "id": "_x910p9enjeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any\n",
        "@dataclass\n",
        "class SARNS():\n",
        "  state: Any\n",
        "  action: Any\n",
        "  reward: Any\n",
        "  next_state : Any\n",
        "\n",
        "\n",
        "class replay(object):\n",
        "  def __init__(self, memory_size):\n",
        "    self.memory = deque([], maxlen = memory_size)\n",
        "    self.capacity = memory_size\n",
        "\n",
        "  def insert(self, SARNS):\n",
        "    #save a tranition\n",
        "    self.memory.append(SARNS)\n",
        "\n",
        "  def samples(self, batch_size): #also used in pritorized replay\n",
        "    batch_size = min(batch_size, len(self))\n",
        "    return sample(self.memory, batch_size)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.memory)"
      ],
      "metadata": {
        "id": "_DO-e-EJTC3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = replay(1000)"
      ],
      "metadata": {
        "id": "7s8jSkfdhV2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(model, state_transitions, tgt, num_actions):\n",
        "      #Get transitions\n",
        "      current_states = torch.stack([torch.tensor(s.state) for s in state_transitions])\n",
        "      rewards = torch.stack([torch.tensor(s.reward) for s in state_transitions])\n",
        "      next_states = torch.stack([torch.tensor(s.next_state) for s in state_transitions])\n",
        "      if_done = torch.stack(\n",
        "          [torch.tensor([0]) if s.done else torch.tensor([1]) for s in state_transitions]\n",
        "                          )\n",
        "      actions = torch.stack([torch.tensor(s.action) for s in state_transitions])\n",
        "\n",
        "\n",
        "      with torch.no_grad():\n",
        "        next_qvals = q_target().max(-1)\n",
        "\n",
        "      model.opt.zero_grad()\n",
        "      quals = model(current_states)\n",
        "      one_hot_actions = F.one_hot(torch.LongTensor(actions), num_actions)\n",
        "\n",
        "      rewards + q_evaluation()"
      ],
      "metadata": {
        "id": "IM2fjQ_4eEeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "  def __init__(self, lr, gamma, n_actions, batch_size, input_dims, epsilon, epsilon_decay = 1e-3, epsilon_min=0.01, memory_size=1000000):\n",
        "    self.action = [action for action in range(n_actions)]\n",
        "    self.gamma = gamma\n",
        "    self.batch_size = batch_size\n",
        "    self.epsilon = epsilon\n",
        "    self.epsilon_min = epsilon_min\n",
        "    self.memory = replay(memory_size)\n",
        "    self.q_eval = DQN...\n",
        "    self.q_tar = DQN\n",
        "    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "  def store_transition():\n",
        "    pass\n",
        "\n",
        "  def choose_action(self, obs):\n",
        "    if np.random.random() < self.epsilon:\n",
        "      action = np.random.choice(self.action.space) #...checkthis\n",
        "    else:\n",
        "      state = \n",
        "      actions = \n",
        "      action = np.argmax(actions)\n",
        "\n",
        "    return action\n",
        "\n",
        "  def learn(self):\n",
        "    #if memory is less than batch size than pass\n",
        "    #else add to batch\n",
        "\n"
      ],
      "metadata": {
        "id": "tEWbRl9w3ehi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#copied varbatim\n",
        "\n",
        "import random\n",
        "\n",
        "class E_Greedy_Policy():\n",
        "    \n",
        "    def __init__(self, epsilon, decay, min_epsilon):\n",
        "        \n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_start = epsilon\n",
        "        self.decay = decay\n",
        "        self.epsilon_min = min_epsilon\n",
        "                \n",
        "    def __call__(self, state):\n",
        "                \n",
        "        is_greedy = random.random() > self.epsilon\n",
        "        \n",
        "        if is_greedy :\n",
        "            # we select greedy action\n",
        "            with torch.no_grad():\n",
        "                Q_network.eval()\n",
        "                # index of the maximum over dimension 1.\n",
        "                index_action = Q_network(state).max(1)[1].view(1, 1).cpu()[0][0].item()\n",
        "                \n",
        "                Q_network.train()\n",
        "        else:\n",
        "            # we sample a random action\n",
        "            index_action = env.action_space.sample()\n",
        "        \n",
        "        return index_action\n",
        "                \n",
        "    def update_epsilon(self):\n",
        "        \n",
        "        self.epsilon = self.epsilon*self.decay\n",
        "        if self.epsilon < self.epsilon_min:\n",
        "            self.epsilon = self.epsilon_min\n",
        "        \n",
        "    def reset(self):\n",
        "        self.epsilon = self.epsilon_start\n",
        "        \n",
        "        \n",
        "policy = E_Greedy_Policy(0.99, decay=0.997, min_epsilon=0.001)"
      ],
      "metadata": {
        "id": "sGdFIeitg_BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_state(state):\n",
        "    state_tensor = torch.tensor(state_tensor, device=device)\n",
        "    return state_tensor"
      ],
      "metadata": {
        "id": "Mv3wTBndkZKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = replay(10000)\n"
      ],
      "metadata": {
        "id": "9DZst2JjkrGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "TARGET_UPDATE = 10\n",
        "\n",
        "# Get screen size so that we can initialize layers correctly based on shape\n",
        "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
        "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
        "#init_screen = get_screen()\n",
        "#_, _, screen_height, screen_width = init_screen.shape\n",
        "\n",
        "# Get number of actions from gym action space\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "policy_net = DQN(24, 48).to(device)\n",
        "target_net = DQN(24, 48).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.SGD(policy_net.parameters(), 0.01)\n",
        "memory = replay(10000)\n",
        "\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "import random\n",
        "import math\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
      ],
      "metadata": {
        "id": "2KWE0neXqT8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_loop():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "      return\n",
        "  \n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "\n",
        "    criterion = nn.mse_loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "      param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "bfEAVs-zoZ_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class choose_epsgre_action():\n",
        "\n",
        "  def __init_(self, epsilon, decay, min_epsilon):\n",
        "    self.epsilon = epsilon\n",
        "    self.decay = decay\n",
        "    self.epsilon_min = min_epsilon\n",
        "\n",
        "  def __call__(self, state):\n",
        "      if random.random() > self.epsilon:\n",
        "        with torch.no_grad():\n",
        "          Q_network.eval()\n",
        "\n",
        "          index_action = Q_network(state).max(1)[1].view(1, 1).cpu()[0][0].item()\n",
        "\n",
        "      else:\n",
        "            # we sample a random action\n",
        "            index_action = random.randint(0,3)\n",
        "        \n",
        "      return index_action\n",
        "\n",
        "  def update_epsilon(self):\n",
        "        \n",
        "        self.epsilon = self.epsilon*self.decay\n",
        "        if self.epsilon < self.epsilon_min:\n",
        "            self.epsilon = self.epsilon_min\n",
        "        \n",
        "  def reset(self):\n",
        "        self.epsilon = self.epsilon_start\n",
        "        \n",
        "        \n",
        "policy = choose_epsgre_action(0.99, decay=0.997, min_epsilon=0.001)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "agdlQGT6WY29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "OBS_SIZE = 24\n",
        "HIDDEN_SIZE = 64\n",
        "ACTION_SIZE = 81\n",
        "\n",
        "Q_network = DQN(OBS_SIZE, HIDDEN_SIZE).to(device)\n",
        "Q_target = DQN(OBS_SIZE, HIDDEN_SIZE).to(device)\n",
        "Q_target.load_state_dict(Q_network.state_dict())\n",
        "Q_target.eval()\n",
        "\n",
        "TARGET_UPDATE = 20\n",
        "\n",
        "optimizer = optim.SGD(Q_network.parameters(), lr=0.01)\n",
        "memory = replay(10000)"
      ],
      "metadata": {
        "id": "7kyGrrvjhFTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
      ],
      "metadata": {
        "id": "FyN1KjDBloeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 256\n",
        "GAMMA = 0.5\n",
        "\n",
        "def optimize_model():\n",
        "    \n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    \n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "    \n",
        "    # Compute Q values using policy net\n",
        "    Q_values = Q_network(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute next Q values using Q_targets\n",
        "    next_Q_values = torch.zeros( BATCH_SIZE, device=device)\n",
        "    next_Q_values[non_final_mask] = Q_target(non_final_next_states).max(1)[0].detach()\n",
        "    next_Q_values = next_Q_values.unsqueeze(1)\n",
        "    \n",
        "    # Compute targets\n",
        "    target_Q_values = (next_Q_values * GAMMA) + reward_batch\n",
        "    \n",
        "    # Compute MSE Loss\n",
        "    loss = F.mse_loss(Q_values, target_Q_values)\n",
        "    \n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    \n",
        "    # Trick: gradient clipping\n",
        "    for param in Q_network.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "        \n",
        "    optimizer.step()\n",
        "    \n",
        "    return loss"
      ],
      "metadata": {
        "id": "4lOiS-M-lU0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "UcZkmaUTsuJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 50\n",
        "for i_episode in range(num_episodes):\n",
        "    # Initialize the environment and state\n",
        "    env.reset()\n",
        "    #last_screen = get_screen()\n",
        "    #current_screen = get_screen()\n",
        "    state = env.reset()\n",
        "    for t in count():\n",
        "        # Select and perform an action\n",
        "        action = select_action(state)\n",
        "        _, reward, done, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "\n",
        "        # Observe new state\n",
        "        last_screen = current_screen\n",
        "        current_screen = get_screen()\n",
        "        if not done:\n",
        "            next_state = current_screen - last_screen\n",
        "        else:\n",
        "            next_state = None\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.push(state, action, next_state, reward)\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization (on the policy network)\n",
        "        optimize_model()\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            plot_durations()\n",
        "            break\n",
        "    # Update the target network, copying all weights and biases in DQN\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "print('Complete')\n",
        "env.render()\n",
        "env.close()\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oHZFLEzJs5Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DQN LOOP"
      ],
      "metadata": {
        "id": "PIrFd0_E5hp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(episodes):\n",
        "  score = 0\n",
        "  done = False\n",
        "  state = env.reset()\n",
        "\n",
        "  while not done:\n",
        "    #choose action\n",
        "    action = agent.choose_action(state) #choose action epsilon greedy action\n",
        "\n",
        "    #commit action\n",
        "    state_, reward, done, info = env.step(action)\n",
        "    score += reward\n",
        "\n",
        "    #store transitions\n",
        "    agent.store_transition(state, action, reward, done, state_)\n",
        "\n",
        "    agent.learn()\n",
        "    state = state_\n",
        "\n"
      ],
      "metadata": {
        "id": "4w-c0tEI5hKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAIN LOOP"
      ],
      "metadata": {
        "id": "fSsMwVlGoY9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEsting"
      ],
      "metadata": {
        "id": "Ro195gA-Jguc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(state_transitions):\n",
        "      #Get transitions\n",
        "      current_states = torch.stack([torch.tensor(s.state) for s in state_transitions])\n",
        "      rewards = torch.stack([torch.tensor(s.reward) for s in state_transitions])\n",
        "      next_states = torch.stack([torch.tensor(s.next_state) for s in state_transitions])\n",
        "      #if_done = torch.stack([torch.tensor([0]) if s.done else torch.tensor([1]) for s in state_transitions])\n",
        "      actions = torch.stack([torch.tensor(s.action) for s in state_transitions])\n",
        "\n",
        "      #with torch.no_grad():\n",
        "        #next_qvals = q_target().max(-1)\n",
        "\n",
        "      #rewards + q_evaluation()\n",
        "\n",
        "      return current_states, rewards, next_states, actions"
      ],
      "metadata": {
        "id": "ID6L3g7yiK1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(model, state_transitions, tgt, num_actions): #target model\n",
        "      #Get transitions\n",
        "      current_states = torch.stack([torch.tensor(s.state, dtype=torch.float32) for s in state_transitions])\n",
        "      rewards = torch.stack([torch.tensor(s.reward) for s in state_transitions])\n",
        "      next_states = torch.stack([torch.tensor(s.next_state, dtype=torch.float32) for s in state_transitions])\n",
        "      mask = torch.stack([torch.tensor([0]) if s.done else torch.tensor([1]) for s in state_transitions])\n",
        "      actions = [torch.tensor(s.action) for s in state_transitions]\n",
        "\n",
        "      #Get Q values on next states from the target network\n",
        "      with torch.no_grad():\n",
        "        qvals_nextstate = tgt(next_states).max(-1)[0] #(N, num_actions)\n",
        "\n",
        "      model.optim.zero_grad()\n",
        "      #Get Q values on current states\n",
        "      qvals = model(current_states)\n",
        "\n",
        "\n",
        "      one_hot_actions = F.one_hot(torch.LongTensor(actions), num_actions)\n",
        "      target_Q_values = (qvals_nextstate * GAMMA) + rewards\n",
        "      loss = ((rewards + mask[:,0] * qvals_nextstate - torch.sum(qvals * one_hot_actions, -1))**2 ).mean()\n",
        "\n",
        "      #compute loss\n",
        "      #loss = F.mse_loss(qvals, target_Q_values)\n",
        "      loss.backward()\n",
        "\n",
        "       #Trick: gradient clipping\n",
        "      for param in model.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "\n",
        "      model.optim.step()\n",
        "      return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "Pfz-j8aZPikH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any\n",
        "@dataclass\n",
        "class SARNS():\n",
        "  state: Any\n",
        "  action: Any\n",
        "  reward: Any\n",
        "  next_state : Any\n",
        "  done: Any\n",
        "\n",
        "\n",
        "class replay(object):\n",
        "  def __init__(self, memory_size=10000):\n",
        "    self.memory = deque([], maxlen = memory_size)\n",
        "    self.capacity = memory_size\n",
        "\n",
        "  def insert(self, SARNS):\n",
        "    #save a tranition\n",
        "    self.memory.append(SARNS)\n",
        "\n",
        "  def samples(self, batch_size): #also used in pritorized replay\n",
        "    batch_size = min(batch_size, len(self))\n",
        "    return sample(self.memory, batch_size)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.memory)"
      ],
      "metadata": {
        "id": "EpqdpsyUOo1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_tgt_model(m, tgt):\n",
        "    tgt.load_state_dict(m.state_dict())\n",
        "\n"
      ],
      "metadata": {
        "id": "9dvtURleO1Tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.observation_space.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKfBYakIezVY",
        "outputId": "2e0f8ac5-086a-4f06-a64f-3ff4c685e8e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24,)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent():\n",
        "    def __init__(self, model):\n",
        "      self.model = model\n",
        "    \n",
        "    def get_actions(self, observations):\n",
        "      return q_vals.max(-1)[1]"
      ],
      "metadata": {
        "id": "CtBw-V4Ivy0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"dqn-no-improvement\", name=\"bot11\")\n",
        "GAMMA = 0.7\n",
        "EPS_DECAY = 0.999999\n",
        "EPS_MIN = 0.01\n",
        "min_rb_size = 10000 #buffer size\n",
        "sample_size = 2500 \n",
        "env_step_before_train = 100\n",
        "tgt_model_update = 50\n",
        "eps = 0.9999\n",
        "\n",
        "last_observation = env.reset() #get observations\n",
        "#last_observation1 = torch.tensor(last_observation, dtype=torch.float32)\n",
        "##m = DQN(24, 81) # evaluation model\n",
        "#tgt = DQN(24, 81) #target model\n",
        "#m = DQN(env.observation_space.shape[0], 81)\n",
        "#tgt = DQN(env.observation_space.shape[0], 81)\n",
        "update_tgt_model(m, tgt)\n",
        "\n",
        "rb = replay(15000) #initialize replay buffer\n",
        "steps_since_train = 0\n",
        "epochs_since_tgt = 0\n",
        "\n",
        "step_num = -1 * min_rb_size\n",
        "\n",
        "eps = 0.9999\n",
        "episode_rewards = []\n",
        "rolling_reward = 0\n",
        "\n",
        "#model loop\n",
        "while True:\n",
        "    \n",
        "    eps = EPS_DECAY ** (step_num)\n",
        "    \n",
        "    if random() < eps:\n",
        "      action = env.action_space.sample()\n",
        "    else:\n",
        "      action = m(torch.Tensor(last_observation)).max(-1)[-1].item()\n",
        "      #action = m(torch.Tensor(last_observation1).item()#your agent here this takes random samples\n",
        "    observation, reward, done, info = env.step(action)\n",
        "\n",
        "    rolling_reward += reward\n",
        "    reward = reward * 0.1\n",
        "\n",
        "    rb.insert(SARNS(last_observation, action, reward, observation, done))\n",
        "\n",
        "    last_observation = observation\n",
        "\n",
        "    if done:\n",
        "      episode_rewards.append(rolling_reward)\n",
        "      rolling_reward = 0\n",
        "      observation = env.reset()\n",
        "\n",
        "\n",
        "    steps_since_train +=1\n",
        "    step_num += 1\n",
        "\n",
        "\n",
        "    if len(rb) > min_rb_size and steps_since_train > env_step_before_train:\n",
        "      loss = training_step(m, rb.samples(sample_size), tgt, 81)\n",
        "      wandb.log({'loss': loss.detach().item(), 'eps':eps, 'avg_reward': np.mean(episode_rewards)}, step = step_num)\n",
        "      print(step_num, loss.detach().item(), np.mean(episode_rewards))\n",
        "      episode_rewards = []\n",
        "      epochs_since_tgt +=1\n",
        "      if epochs_since_tgt > tgt_model_update:\n",
        "        update_tgt_model(m, tgt)\n",
        "        print('updating model', epochs_since_tgt)\n",
        "        epochs_since_tgt = 0\n",
        "      steps_since_train = 0\n",
        "      print(loss)\n"
      ],
      "metadata": {
        "id": "EHbcdfh8MA1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Double DQN"
      ],
      "metadata": {
        "id": "UQS-RFY93Bzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, obs_dim, size, batch_size):\n",
        "        self.observations_buffer = np.zeros([size, obs_dim])\n",
        "        self.next_obs_buf = np.zeros([size, obs_dim])\n",
        "        self.actions_buffer = np.zeros([size])\n",
        "        self.rewards_buffer = np.zeros([size])\n",
        "        self.done_buffer = np.zeros(size)\n",
        "        self.max_size, self.batch_size = size, batch_size\n",
        "        self.ptr, self.size, = 0, 0\n",
        "\n",
        "    def store(self, obs, act, rew, next_obs, done):\n",
        "        self.observations_buffer[self.ptr] = obs\n",
        "        self.next_obs_buf[self.ptr] = next_obs\n",
        "        self.actions_buffer[self.ptr] = act\n",
        "        self.rewards_buffer[self.ptr] = rew\n",
        "        self.done_buffer[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample_batch(self):\n",
        "        idx = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
        "        return dict(observations=self.observations_buffer[idx],\n",
        "                    next_observations=self.next_obs_buf[idx],\n",
        "                    actions=self.actions_buffer[idx],\n",
        "                    rewards=self.rewards_buffer[idx],\n",
        "                    done=self.done_buffer[idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size"
      ],
      "metadata": {
        "id": "e_6AcsC31crV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        \"\"\"Initialization.\"\"\"\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(in_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self,env: gym.Env,memory_size: int,batch_size: int,target_update: int,epsilon_decay: float,max_epsilon: float = 1.0,min_epsilon: float = 0.1,gamma: float = 0.99,):\n",
        "        \n",
        "        obs_dim = env.observation_space.shape[0]\n",
        "        action_dim = env.action_space.n\n",
        "\n",
        "        self.env = env\n",
        "        self.memory = ReplayBuffer(obs_dim, memory_size, batch_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.epsilon = max_epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.max_epsilon = max_epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.target_update = target_update\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # device: cpu / gpu\n",
        "        self.device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "\n",
        "        # networks: dqn, dqn_target\n",
        "        self.dqn = Network(obs_dim, action_dim).to(self.device)\n",
        "        self.dqn_target = Network(obs_dim, action_dim).to(self.device)\n",
        "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
        "        self.dqn_target.eval()\n",
        "\n",
        "        # optimizer\n",
        "        self.optimizer = optim.Adam(self.dqn.parameters())\n",
        "\n",
        "        # transition to store in memory\n",
        "        self.transition = list()\n",
        "\n",
        "\n",
        "    def select_action(self, state: np.ndarray) ->:\n",
        " \n",
        "        # epsilon greedy policy\n",
        "        if self.epsilon > np.random.random():\n",
        "            selected_action = self.env.action_space.sample()\n",
        "        else:\n",
        "            selected_action = self.dqn(\n",
        "                torch.FloatTensor(state).to(self.device)\n",
        "            ).argmax()\n",
        "            selected_action = selected_action.detach().cpu().numpy()\n",
        "\n",
        "        self.transition = [state, selected_action]\n",
        "\n",
        "        return selected_action\n",
        "\n",
        "    def step(self, action: np.ndarray):\n",
        "        #action and next state and reward\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "\n",
        "        self.transition += [reward, next_state, done]\n",
        "        self.memory.store(*self.transition)\n",
        "\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def update_model(self) :\n",
        "        samples = self.memory.sample_batch()\n",
        "\n",
        "        loss = self._compute_dqn_loss(samples)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def train(self, num_frames: int, plotting_interval: int = 200):\n",
        "\n",
        "        state = self.env.reset()\n",
        "        update_cnt = 0\n",
        "        epsilons = []\n",
        "        losses = []\n",
        "        scores = []\n",
        "        score = 0\n",
        "\n",
        "        for frame_idx in range(1, num_frames + 1):\n",
        "            action = self.select_action(state)\n",
        "            next_state, reward, done = self.step(action)\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            # if episode ends\n",
        "            if done:\n",
        "                state = self.env.reset()\n",
        "                scores.append(score)\n",
        "                score = 0\n",
        "\n",
        "            # if training is ready\n",
        "            if len(self.memory):\n",
        "                loss = self.update_model()\n",
        "                losses.append(loss)\n",
        "                update_cnt += 1\n",
        "\n",
        "                # epsilon decay\n",
        "                self.epsilon = max(\n",
        "                    self.min_epsilon, self.epsilon - (\n",
        "                            self.max_epsilon - self.min_epsilon\n",
        "                    ) * self.epsilon_decay\n",
        "                )\n",
        "                epsilons.append(self.epsilon)\n",
        "\n",
        "                # if hard update is needed\n",
        "                if update_cnt % self.target_update == 0:\n",
        "                    self._target_hard_update()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _compute_dqn_loss(self, samples: Dict[str, np.ndarray]) -> torch.Tensor:\n",
        "        \"\"\"Return dqn loss.\"\"\"\n",
        "        device = self.device  # for shortening the following lines\n",
        "        state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
        "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
        "        action = torch.LongTensor(samples[\"acts\"].reshape(-1, 1)).to(device)\n",
        "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
        "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
        "\n",
        "\n",
        "        curr_q_value = self.dqn(state).gather(1, action)\n",
        "        next_q_value = self.dqn_target(next_state).gather(  # Double DQN\n",
        "            1, self.dqn(next_state).argmax(dim=1, keepdim=True)\n",
        "        ).detach()\n",
        "        mask = 1 - done\n",
        "        target = (reward + self.gamma * next_q_value * mask).to(self.device)\n",
        "\n",
        "        # calculate dqn loss\n",
        "        loss = F.smooth_l1_loss(curr_q_value, target)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def _target_hard_update(self):\n",
        "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n"
      ],
      "metadata": {
        "id": "QL64_4942eF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deuling DQN"
      ],
      "metadata": {
        "id": "97O67fH72Csi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, observations_dim, output_dim):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        # set common feature layer\n",
        "        self.common_stream = nn.Sequential(\n",
        "            nn.Linear(observations_dim, 128),\n",
        "            nn.ReLU())\n",
        "\n",
        "        #advantage function for each action a =  (A(s, a))\n",
        "        self.advantage_stream = nn.Sequential(\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim))\n",
        "\n",
        "        # #state value function for state s =  (V(s))\n",
        "        self.value_stream = nn.Sequential(\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        feature = self.common_stream(x)\n",
        "\n",
        "        value = self.value_stream(feature)\n",
        "        advantage = self.advantage_stream(feature)\n",
        "        # infinitely many solutions so trick to make problem identifiable\n",
        "        # we take mean\n",
        "        q = value + advantage - advantage.mean(dim=-1, keepdim=True)\n",
        "\n",
        "        return q\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        " \n",
        "    def __init__( self, env, memory_size, batch_size, target_update, epsilon_decay, max_epsilon = 1.0, min_epsilon = 0.1,gamma = 0.99,):\n",
        "\n",
        "        obs_dim = env.observation_space.shape[0]\n",
        "        action_dim = env.action_space.n\n",
        "\n",
        "        self.env = env\n",
        "        self.memory = ReplayBuffer(obs_dim, memory_size, batch_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.epsilon = max_epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.max_epsilon = max_epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.target_update = target_update\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # device: cpu / gpu\n",
        "        self.device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # networks: dqn, dqn_target\n",
        "        self.dqn = Network(obs_dim, action_dim).to(self.device)\n",
        "        self.dqn_target = Network(obs_dim, action_dim).to(self.device)\n",
        "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
        "        self.dqn_target.eval()\n",
        "\n",
        "        # optimizer\n",
        "        self.optimizer = optim.Adam(self.dqn.parameters())\n",
        "\n",
        "        # transition to store in memory\n",
        "        self.transition = list()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        # epsilon greedy policy\n",
        "        if self.epsilon > np.random.random():\n",
        "            selected_action = self.env.action_space.sample()\n",
        "        else:\n",
        "            selected_action = self.dqn(\n",
        "                torch.FloatTensor(state).to(self.device)\n",
        "            ).argmax()\n",
        "            selected_action = selected_action.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "        self.transition = [state, selected_action]\n",
        "\n",
        "        return selected_action\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "        self.transition += [reward, next_state, done]\n",
        "        self.memory.store(*self.transition)\n",
        "\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def update_model(self):\n",
        "        samples = self.memory.sample_batch()\n",
        "\n",
        "        loss = self._compute_dqn_loss(samples)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # trick: gradient clipping.\n",
        "        clip_grad_norm_(self.dqn.parameters(), 10.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def train(self, num_steps, plotting_interval = 200):\n",
        "#train agent\n",
        "        state = self.env.reset()\n",
        "        update_target_count = 0\n",
        "        epsilons, losses, scores = [], [], []\n",
        "        total_ep_reward = 0\n",
        "\n",
        "        for steps in range(num_steps):\n",
        "            action = self.select_action(state)\n",
        "            next_state, reward, done = self.step(action)\n",
        "\n",
        "            state = next_state\n",
        "            total_ep_reward += reward\n",
        "\n",
        "            # if episode ends\n",
        "            if done:\n",
        "                state = self.env.reset()\n",
        "                scores.append(total_ep_reward)\n",
        "                total_ep_reward = 0\n",
        "\n",
        "            # if training is ready\n",
        "            if len(self.memory) >= self.batch_size:\n",
        "                loss = self.update_model()\n",
        "                losses.append(loss)\n",
        "                update_target_count += 1\n",
        "\n",
        "                #epsilon decay\n",
        "                self.epsilon = max(self.min_epsilon, self.epsilon - (self.max_epsilon - self.min_epsilon) * self.epsilon_decay)\n",
        "                epsilons.append(self.epsilon)\n",
        "\n",
        "                # if hard update is needed\n",
        "                if update_target_count % self.target_update == 0:\n",
        "                    self.update_target_function()\n",
        "\n",
        "\n",
        "    def _compute_dqn_loss(self, samples):\n",
        "        \"\"\"Return dqn loss.\"\"\"\n",
        "        device = self.device\n",
        "        state = torch.FloatTensor(samples[\"observations\"]).to(device)\n",
        "        next_state = torch.FloatTensor(samples[\"next_observations\"]).to(device)\n",
        "        action = torch.LongTensor(samples[\"actions\"].reshape(-1, 1)).to(device)\n",
        "        reward = torch.FloatTensor(samples[\"rewards\"].reshape(-1, 1)).to(device)\n",
        "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
        "\n",
        "        curr_q_value = self.dqn(state).gather(1, action)\n",
        "        next_q_value = self.dqn_target(next_state).max(dim=1, keepdim=True)[0].detach()\n",
        "        mask = 1 - done\n",
        "        target = (reward + self.gamma * next_q_value * mask).to(self.device)\n",
        "\n",
        "        # calculate dqn loss\n",
        "        loss = F.smooth_l1_loss(curr_q_value, target)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    #update target network\n",
        "    def update_target_function(self):\n",
        "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
        "\n",
        "\n",
        "#training loop\n",
        "\n",
        "state = env.reset()\n",
        "update_target_count = 0\n",
        "epsilons, losses, reward_list = [], [], []\n",
        "total_ep_reward = 0\n",
        "\n",
        "for steps in range(num_steps):\n",
        "    action = select_action(state)\n",
        "    next_state, reward, done = step(action)\n",
        "\n",
        "    state = next_state\n",
        "    total_ep_reward += reward\n",
        "\n",
        "    # if episode ends\n",
        "    if done:\n",
        "        state = env.reset()\n",
        "        reward_list.append(total_ep_reward)\n",
        "        total_ep_reward = 0\n",
        "\n",
        "    # if training is ready\n",
        "    if len(memory) >= batch_size:\n",
        "        loss = update_model()\n",
        "        losses.append(loss)\n",
        "        update_target_count += 1\n",
        "\n",
        "        #epsilon decay\n",
        "        epsilon = max(min_epsilon, epsilon - (max_epsilon - min_epsilon) * epsilon_decay)\n",
        "        epsilons.append(epsilon)\n",
        "\n",
        "        # target update\n",
        "        if update_target_count % target_update == 0:\n",
        "            update_target_function()"
      ],
      "metadata": {
        "id": "w5ldWqW72BXJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}